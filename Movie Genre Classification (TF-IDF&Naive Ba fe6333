{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2347441,"sourceType":"datasetVersion","datasetId":1417162}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"Movie Genre Classification (TF-IDF&Naive Ba fe6333","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/movie-genre-classification-tf-idf-naive-ba-fe6333-daae85c5-158a-4096-b88a-82641f33fc33.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240506/auto/storage/goog4_request&X-Goog-Date=20240506T151222Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=5ecfecae47dd31207dcdf002cd7f75944e0114c3d65bac26b1538affd178610596c8d534af6ab806199cfae6f5b389c972bd3b48074bcd95f57a4013cc9519bc90c887ca25a3212336e4f0381f9b14b1f196659e9245fe27567148f2c414d5a94cfefdb22aa2846a5d64d0dc0d937a610f0a5f1e0a34976c4531b55a1eaff46aecdf68d6846cab3c1ecb210520a42d1809aba112dcb2db3e5ce7800d82147a5172ff5263c3805c136a5883075b93b5694a93037f9dee342f30cf2bc81e9271211c297d7cd7ec939f253cde16b39b92247a7fc9fa30c34b6451ac20f42ed23b30c78bfd8a9ae7ee2b4b20c854c6ba9ee3d0e19d53b193ad5d852129466a8d2605","timestamp":1715008451031}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'genre-classification-dataset-imdb:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1417162%2F2347441%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240506%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240506T151221Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D54c5cde7781a8ca2b10f88ae0b885843f0ab46efcc5f9621f1daeb031603cb9409559afde68c6ce6e364ea62e59c7e0ff7354b7b3c5ebc6dd050cff13f4f19e479cb5383db461f0e5fa3a5d84302b520ff7bad4b6f75b6af6883ff9fc1e676dd20c03807f0a4822e5b092080f47cfa2c5d9b309d3e29e13a9d64a469624c47c93f10debdeef4fd9eb62e4007a76e00f02a3a9ef20972258c83f968921623455127a3e48df6a77e455fd466e019aab66b9e96146ecff09c790cde62569c61dea8f08f162167edb991af7029b57e52deaabd602d21c5fbaba2bffa3960d5b065750c59a898264c00ce6ff71d28630fa1e0712e03b47a740fa73ae0e728045e9dbd'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')"],"metadata":{"id":"qUStzS3Hdqtm"},"cell_type":"code","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"nOe9mhJudquC"}},{"cell_type":"code","source":[],"metadata":{"id":"LXIisfPgdquN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set(rc={'figure.figsize':(18,8)},style='darkgrid')\n","from time import time\n","import re\n","import string\n","import nltk\n","from googletrans import Translator\n","from langdetect import detect\n","import pycountry\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import *\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Lg625xOTdquP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Collect Data"],"metadata":{"id":"ogq2UTNfdquT"}},{"cell_type":"code","source":["train = pd.read_csv(r\"E:\\Data Science\\Datasets\\Genre Classification Dataset\\train_data.txt\",\n","                    sep=':::',names=['Title', 'Genre', 'Description']).reset_index(drop=True)\n","train.head()"],"metadata":{"id":"V8jxoEkCdquW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv(r\"E:\\Data Science\\Datasets\\Genre Classification Dataset\\test_data.txt\",\n","                  sep=':::',names=['Title', 'Description']).reset_index(drop=True)\n","test.head()"],"metadata":{"id":"Lztb_EPYdquY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Cleaning"],"metadata":{"id":"W_nW5eoddquc"}},{"cell_type":"markdown","source":["### For Train Data\n"],"metadata":{"id":"6vCkU88cdque"}},{"cell_type":"code","source":["train.describe(include='object').T"],"metadata":{"id":"RiUgYTI4dquh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.info() #No null values"],"metadata":{"id":"p2RgY8ZXdquk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.duplicated().sum() #No duplicates"],"metadata":{"id":"JFl8GI8jdqum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.Genre.unique() #No anomalies values"],"metadata":{"id":"sdKYLmm-dqup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### For Test Data"],"metadata":{"id":"whWPkl0Hdqus"}},{"cell_type":"code","source":["test.describe(include='object').T"],"metadata":{"id":"SFXHMVzXdquu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.info() #No null values"],"metadata":{"id":"1j3Rs2Vpdqux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.duplicated().sum() #No duplicates"],"metadata":{"id":"j9bXB6S2dquz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory Data Analysis"],"metadata":{"id":"RVtyZ21hdqu2"}},{"cell_type":"markdown","source":["### Text Cleaning"],"metadata":{"id":"h1CCsbi0dqu4"}},{"cell_type":"markdown","source":["**Searching for anomalies in description:**\n","- Punctuations\n","- HTTP\n","- Numbers\n"],"metadata":{"id":"DgGd-7mSdqu5"}},{"cell_type":"code","source":["train.loc[train['Description'].str.contains(r'@\\S+')].head()"],"metadata":{"id":"HX4CIeoCdqu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#We discover that there is more than one language so we need to handle that\n","#First we need to discover different languages\n","def detect_language(text):\n","    try:\n","        #Returning the name 'English instide of en'\n","        return pycountry.languages.get(alpha_2=detect(text)).name.lower()\n","    except:\n","        return 'Unknown'\n","train['Language'] = train['Description'].apply(detect_language)\n","test['Language'] = test['Description'].apply(detect_language)\n","train.head()"],"metadata":{"id":"YUjYP8NRdqu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2)\n","axs[0].pie(train.Language.value_counts().values.tolist(),autopct='%.2f%%')\n","axs[0].legend(labels=train.Language.value_counts().index.tolist(),loc='lower left')\n","sns.countplot(data=train,y='Language',order=train.Language.value_counts().index.tolist(),ax=axs[1],color='lightblue')\n","axs[1].bar_label(axs[1].containers[0])\n","fig.show()"],"metadata":{"id":"k93rTjQHdqvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#We discovered a portion of other languages in our data, so we will handle\n","#them later by translating them to standard language (english) in our cleaning function"],"metadata":{"id":"Zloi5jxodqvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Descriptions included HTTP links\n","train.loc[train['Description'].str.contains(r'http\\S+')].shape[0]"],"metadata":{"id":"bfi-GpIldqvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For example\n","train.loc[train['Description'].str.contains(r'http\\S+')].head()['Description'].iloc[1]\n","#So we need to remove them from our text"],"metadata":{"id":"TtZS8MV-dqvI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Cleaning Text Function"],"metadata":{"id":"c8oFPOHsdqvL"}},{"cell_type":"code","source":["def clean_text(text):\n","    # Remove strange pattern in different languages if exist\n","    text = re.sub('Mail <svaradi@sprynet.com> for translation. ','',text)\n","    # Remove twitter handles\n","    text = re.sub(r'@\\S+', '', text)\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","    # Remove punctuations\n","    text = re.sub(f'[{string.punctuation}]','',text)\n","    # Remove numbers\n","    text = re.sub(f'[{string.digits}]','',text)\n","    # Remove single charachters\n","    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n","\n","    return text"],"metadata":{"id":"1QyMo3nbdqvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Clean Descriptions\n","train['Description'] = train['Description'].apply(clean_text)\n","test['Description'] = test['Description'].apply(clean_text)"],"metadata":{"id":"77xQWCz1dqvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train[~train.Language.isin(['english'])]"],"metadata":{"id":"pmqbdAQ4dqvS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Translate other languages\n","def trans(text):\n","    try:\n","        return Translator().translate(text,dest='en').text\n","    except:\n","        return text\n","train.loc[~train['Language'].isin(['english']), 'Description']=train.loc[~train['Language'].isin(['english']),'Description'].apply(trans)\n","test.loc[~test['Language'].isin(['english']), 'Description']=test.loc[~test['Language'].isin(['english']),'Description'].apply(trans)"],"metadata":{"id":"Qrzv5tahdqvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.drop(columns='Language',inplace=True)\n","test.drop(columns='Language',inplace=True)"],"metadata":{"id":"M10gP9GadqvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Distribution of text lengths\n","train['Dest_len'] = train['Description'].apply(len)\n","sns.distplot(train['Dest_len'])\n","plt.title('Distribution of lengths',fontweight='bold',fontsize=20)\n","plt.show()"],"metadata":{"id":"iJrj5GoDdqvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ax=sns.countplot(data=train,x='Genre',order=train.Genre.value_counts().index,palette='rocket')\n","ax.bar_label(ax.containers[0])\n","plt.title('Genres Distribution',fontweight='bold',fontsize=16)\n","plt.xticks(rotation=45,fontweight='bold',fontsize=10)\n","plt.show()"],"metadata":{"id":"xrqPwxbQdqvc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Building"],"metadata":{"id":"QMzvVXwCdqvf"}},{"cell_type":"code","source":["# Using TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(lowercase=True, #Lowercase chars\n","                                   ngram_range=(1,1), #Capture only single words in each text(unigrams)\n","                                   stop_words='english',#Remove stop_words\n","                                   min_df=2)#Ignore words that appears less than 2 times\n","x_train = tfidf_vectorizer.fit_transform(train['Description'])\n","x_test = tfidf_vectorizer.transform(test['Description'])\n"],"metadata":{"scrolled":true,"id":"vIu1mVOzdqvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#We conclude before that drama and documentary have the majority of our data,\n","#so to avoid imbalance data in our model we will make randomoversampling\n","#Notice that the accuracy before sampling will be < the accuracy after oversampling\n","sampler = RandomOverSampler()\n","#We will pass to it the output of TfidfVectorizer from train data\n","x_train_resampled , y_train_resampled = sampler.fit_resample(x_train,train['Genre'])"],"metadata":{"id":"ASsB3OYadqvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Let's take a look on genre distribution\n","sns.countplot(data=y_train_resampled,x=y_train_resampled.values,palette='rocket')\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"ZycmlFnMdqvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Double check for length of our data\n","print('Train :',x_train_resampled.shape[0])\n","print('Test :',y_train_resampled.shape[0])"],"metadata":{"id":"vTWJ89Lydqvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Get the actual solutions to compare it with our predictions\n","y_actual = pd.read_csv(r\"E:\\Data Science\\Datasets\\Genre Classification Dataset\\test_data_solution.txt\",\n","                      sep=':::',usecols=[2],header=None).rename(columns={2:'Actual_Genre'})\n","y_actual.head()"],"metadata":{"id":"yGqDgdB5dqwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Naive Bayes Model\n","NB = MultinomialNB(alpha=0.3)\n","start_time = time()\n","NB.fit(x_train_resampled,y_train_resampled)\n","y_pred = NB.predict(x_test)\n","print('Accuracy :',accuracy_score(y_actual,y_pred))\n","end_time = time()\n","print('Running Time : ',round(end_time - start_time,2),'Secounds')"],"metadata":{"id":"Q4kr0ZzgdqwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_actual,y_pred))"],"metadata":{"id":"eC1mN4mPdqwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm =confusion_matrix(y_actual,y_pred,labels=NB.classes_)\n","cmd = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=NB.classes_)\n","cmd.plot(cmap=plt.cm.Reds,xticks_rotation='vertical',text_kw={'size': 8})\n","plt.show()"],"metadata":{"id":"ZvcyHiHYdqwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.concat([pd.concat([test,y_actual],axis=1),pd.Series(y_pred)],axis=1).rename(columns={0:'Predicted_Genre'}).head(10)"],"metadata":{"id":"CXPXs_ondqwW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Another approach to inhance accuracy"],"metadata":{"id":"tQujerMhdqwY"}},{"cell_type":"code","source":["#We got low accuracy due to insufficient data for other categories\n","#So the model trained alot about drama and documentary movies so it's hard to discover the others\n","#Now let's try another approach and see the effect on data\n","y_train_modified = train['Genre'].apply(lambda genre: genre if genre.strip() in ['drama','documentary'] else 'other')\n","y_actual_modified = y_actual['Actual_Genre'].apply(lambda genre: genre if genre.strip() in ['drama','documentary'] else 'other')"],"metadata":{"id":"H_ESOpMvdqwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NB = MultinomialNB(alpha=0.3)\n","start_time = time()\n","NB.fit(x_train,y_train_modified)\n","y_pred = NB.predict(x_test)\n","print('Accuracy :',accuracy_score(y_actual_modified,y_pred))\n","end_time = time()\n","print('Running Time : ',round(end_time - start_time,2),'Secounds')"],"metadata":{"id":"Zh9HGt4Rdqwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### As Expected! the accuracy increased since the model can capture drama and documentary genres clearly and make (other) for another genres"],"metadata":{"id":"BpXgyiKidqwg"}}]}